---
title: Viam Application End-to-End Flow
summary: Overview of application end-to-end request flow in Viam's robotics architecture
authors:
    - Matt Dannenberg
date: 2022-04-08
---
# Application end-to-end flow

## Running a robot on the RDK
When the RDK starts, it uses the secret in its cloud configuration file to ask app.viam.com for its robot configuration (see Runtime architecture).

Next, the configuration is parsed and processed section by section specified in the JSON config fields, notably remotes, components, services, and processes.

#### Remotes
Remotes represent a connection to another robot part comprising the whole robot.

Initializing a remote involves establishing a network connection to that robot either over direct gRPC or gRPC via WebRTC.

Once established, the part making the connection will request information from the remote part about what components and services it offers (resource discovery) and from there on out will treat those resources as it treats its own local resources. That is, a user connecting to this part will see the components and services as if they were a resource of this part.

This allows for creating a single part that can handle or direct all operations/commands sent to it, even those components which belong to another part, providing for various network and compute topologies.

#### Processes
Processes are simply binaries or scripts that the RDK will run either once or indefinitely and maintain for the lifetime of the RDK. Two typical examples are: one, running an SDK server like the Python SDK where the implementation of a component is easier to create than the RDK; and two, a camera server that has the appropriate system driver to talk to the camera and communicate results over the wire.

#### Components and Services
Both components and services are part of the resource hierarchy and the RDK is packaged with a supported set of implementations (similar to drivers) for each component and service.

###### Components
The components are by default initialized in the order they are specified in the JSON config file fetched from the cloud. Changing the initialization order involves use of the depends_on field to create dependency relationships between components.

Initializing a component consults the component subtype (e.g. arm) and the model of the component (e.g. ur5e) in the packaged registry on how to construct and configure it.

Each component will have access to the other components that it depends on when it gets constructed. As components get reconfigured, the handle that one component has on another is kept intact for uninterrupted use.

###### Services
Once components are initialized, service initialization begins and strictly follows the order that the services were listed in.

## Low-Level Inter-Robot/SDK Communication
Every component and service type in the RDK, and the Viam API for that matter, are represented as Protocol Buffers (protobuf) services. protobuf is a battle tested Interface Description Language (IDL) that allows for specifying services, their methods, and the messages that comprise those methods. Code that uses protobuf is autogenerated and compiles messages into a binary form.

gRPC is responsible for the transport and communication of protobuf messages when calling protobuf methods. It generally works over a TCP, TLS backed HTTP2 connection operating over framing (https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md).

The RDK uses protobuf and grpc to enable access and control to its components and services. That means if there are two arms in a robot configuration, there is only one Arm service that handles the Remote Procedure Calls (RPC) for all arms configured.

In addition to gRPC, the RDK uses WebRTC video & audio streams and data channels to enable peer to peer (P2P) communication between robot parts as well as SDKs and the Remote Control interface.

An outline of how WebRTC is utilized lives here, but in short, an RDK is always waiting on app.viam.com to inform it of a connection requesting to be made to it whereby it sends details about itself and how to connect on a per connection basis. Once a connection is made, app.viam.com is no longer involved in any packet transport and leaves it up to the two peers to communicate with each other.

## High-Level Inter-Robot/SDK Communication
Let's assume there is the following robot part topology

RobotPart1 - The main part

Contains a single USB connected camera called camera1

RobotPart2

Contains an arm stationed on a gantry, respectively called arm1 and gantry1.

RobotPart1 will establish a bidirectional gRPC/WebRTC connection to RobotPart2

RobotPart1 is considered the controlling peer (client)

RobotPart2 is consider the controlled peer (server)

Let's say that we are a user using an SDK wants to have the camera track the largest object in the scene and instruct the arm to track the same object.

Since RobotPart1 is the main part and has access to all other parts, we will connect to that using the SDK.

Once connected, we generally want to do the following series of actions:

Get segmented point clouds from the camera and the object segmentation service.

Find the largest object by volume.

Take the object's center pose and tell the motion service to move the arm to that point.

Go back to 1.

Let's breakdown how each of these steps actually works

Get segmented point clouds from the camera and the object segmentation service.

2. The SDK will send a GetObjectPointClouds request with camera1 being referenced in the message to RobotPart1's Object Segmentation Service.

2. RobotPart1's Object Segmentation Service will receive this message and have its GetObjectPointCloud's method called.

2. It will look up the camera referenced, call the GetObjectPointCloud method on it, and use a point cloud segmentation algorithm to segment geometries in the pointcloud.
Note: The points returned are respective to the reference frame of the camera. This will be important in a moment.

2. The set of segmented point clouds and their bounding geometries are sent back to us.

Find the largest object by volume.

2. We iterate over the geometries of the segmented point clouds returned to us and simply find the object with the max volume and record its center pose.

Take the object's center pose and tell the motion service to move the arm to that point.

2. The SDK will send a Move request for arm1 to the Motion service on RobotPart1 with the destination set to the center point we saved in step 2.

2. RobotPart1's Motion service will receive this message and have its Move method called.

2. First it will look up the arm to move to verify it exists. It will since it comes from the remote RobotPart2.

2. Now the frame system comes into play. Our center pose came from the camera but we want to move the arm to that position even though the arm lives in its own frame. The frame system logic in the RDK automatically handles the transformation logic between these two reference frames while also handling understanding how the frame systems are connected across the two parts.

2. Once a pose in the reference frame of the arm is computed, the motion service can take this pose, and begin to create a plan on how to move the arm in addition to the gantry to achieve this new goal pose. The plan will consist of a series of movements that combine inverse kinematics, mathematics and constraints based motion planning to eventually get the arm and gantry to their goal positions.

2. In executing the plan, which is being coordinated on RobotPart1, it will send messages to the Arm and Gantry services on RobotPart2 for arm1 and gantry1 respectively. RobotPart2 will be unaware of the actual plan and instead will only receive distinct messages to move the components individually.

2. In order for RobotPart1's motion planning to keep track of the overall movements and their completion, it can make use of RobotPart2's status service to find out where the arm and gantry are.
